LXC provision
======================

As always, the configuration is done via a registry: :ref:`module_mc_lxc`.

To provision a new lxc provider, you need to:

    - Select a profile type (combination of backing mode (lvm/dir) and install
      mode(clone/create)
    - Select a size profile if you are using lvm
    - Select a mode: mastersalt (localmaster+minion and a minion linked to
      mastersalt) or salt (only a minion linked to master)
    - add an entry to your lxc container in pillar reflecting those choices plus
      the other vm parameters.

* We create a default container in mastersalt node named **makina-states-precise**. In
  cloning mode, this is the default origin container. Its oip is **10.5.0.2**.
* You can either start an lxc from scratch or begin with a template like a barebone ubuntu which will be cloned at a start.
* By default, we use the **10.5/16** network for all containers
* This use the **lxcbr1** bridge.
* The default gateway is **10.5.0.1**
* We use **LVM** as the default backing store, and the **data** volume group.
* Salt cloud profiles are just collections of default for your next provision.

 * The naming scheme of raw salt-cloud is ms-**{encoded minion id}**[-**{size profile}**]-**{profile type}**, eg profiles::

    ms-devhost-10-local-dir-scratch
    ms-devhost-10-local-dir
    ms-devhost-10-local-small-lvm-scratch
    ms-devhost-10-local-medium-lvm

* Those are the availables modes:

    :salt: cf saltity modes
    :mastersalt: cf saltity modes

* You can specify the makina-states branch to use with:

   :bootsalt_branch: branch name

* Those are the sizes available in profiles

        :xxxtrem: 2000g
        :xxtrem: 1000g
        :xtrem: 500g
        :xxxlarge: 100g
        :large: 20g
        :medium: 10g
        :small: 5g
        :xsmall: 3g
        :xxsmall: 1g
        :xxxsmall: 500m

* Those are the types available in salt-cloud profiles

    :lvm-sratch: starting a lxc container from scratch (lvm backing)
    :dir-scratch:  starting a lxc container from scratch (directory backing)
    :lvm (default): cloning from existing container (lvm backing)
    :dir: cloning from existing container (directory backing)


* Attention, we need also that root can connect via ssh to the box as root user.
  Please not that the mc_lxc.sync_images will do that setup for you and is
  needed to be runned prior installing containers on a specific hosts.

But you can more easily use pillar entries to define each of your coantaners as
follow:

.. code-block:: yaml

  makina-states.services.virt.lxc.containers.devhost10.local:
    mysupertest4:
      name: gfoobar.test.com
      profile: small
      profile_type: lvm
      password: foobar
      ip: 10.5.10.16
    mysupertest4:
      name: gfoobar2.test.com
      ip: 10.5.10.15
      profile: small
      mode: mastersalt
      profile_type: lvm
      password: foobar
    mysupertest6:
      name: gfoobar2.test.com
      ip: 10.5.10.17
      # from_container: makina-states-precise -> default
      profile_type: dir
      mode: mastersalt
      password: foobar
      bootsalt_branch: stable
    mysupertest6:
      name: gfoobar2.test.com
      ip: 10.5.10.17
      # image: ubuntu -> default
      profile_type: dir-scratch
      mode: mastersalt
      password: foobar

* The first line ends (after **containers.**) with your targeted minion id, where the lxc containers will be installed.
* You have to assign the ip yourself to something that will be in the **10.5/16** network or the targeted minion
* MAC is autogenerated, so you dont need to give one
* And the inner mappings define the container themselves.
* Please note that the name in makina-corpus way must be the NickName FQDN.
* Please do not use **snapshot** in production.

to destroy at once boxes and minion keys on master::

    salt-cloud -d <name>

