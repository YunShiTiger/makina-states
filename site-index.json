
[    {"url": "/tags/ansible/", "title": "Ansible",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/topics/ansible/", "title": "Ansible related configuration",  "subtitle": "", "section": "topics", "tags": ["topics", "ansible"], "type": "topics", "description": "", "content": "Ansible wrappers specifics To use ansible, please use makina-states wrappers and never the ansible original scripts directly. See: bin/ansible bin/ansible-galaxy bin/ansible-playbook bin/ansible-wrapper-common We preconfigure in our wrappers a lot of things like: Loading configuration (roles, playbooks, inventories, plugins) from: ./ ./ansible ./.ansible \u0026lt;makinastates_install_dir\u0026gt;/ansible /usr/share/ansible (depends of the opt, respects the ansible default configuration) /etc/ansible (depends of the opt, respects the ansible default configuration) When ANSIBLE_TARGETS are set, we will limit the play to them unless ANSIBLE_NOLIMIT is set. Indeed, we use environment variables that are specific to makina-states: ANSIBLE_TARGETS : list of hosts that we will act on. this will limit the scope of the ext pillars generation thus you have to set it to speed up operations. ANSIBLE_NOLIMIT : if set, we wont limit the scope of ansible to ANSIBLE_TARGETS Examples exemple 1 ANSIBLE_TARGETS=$(hostname) bin/ansible all -m ping exemple 2 bin/ansible -c local -i \u0026quot;localhost,\u0026quot; all -m ping Calling makina-states ansible from another repository As said previously, we load the current folder (and ./.ansible, ./ansible as well), thus you can add roles and plays and to a specific repository but also depend on plugins or roles defined in makina-states. Then you ll have to call the ansible wrapper FROM your directory where you have your specific ansible installation for them to be usable For example: if makina-states is installed in /srv/makina-states your project is installed inside /srv/projects/foo/project You can create your roles inside /srv/projects/foo/project/ansible/roles You can make dependencies of any makina-states roles specially makinastates_pillar. To call ansible, do it this way: cd /srv/projects/foo/project /srv/makina-states/bin/{ansible,ansible-playbook} $args saltcall Wrapper We developped a special module to call saltcall on remote systems. You can use it via ansible: ANSIBLE_TARGETS=$(hostname) bin/ansible all \\ -m saltcall -a \u0026quot;function=test.ping\u0026quot; ANSIBLE_TARGETS=$(hostname) bin/ansible all \\ -m saltcall -a \u0026quot;function=grains.get args=fqdn\u0026quot; Or via a playbook like in our saltcall one , usable this way: ```sh ANSIBLE_TARGETS=$(hostname) bin/ansible-playbook \\ ansible/plays/saltcall.yml -m saltcall -a \u0026quot;function=test.ping\u0026quot; ``` It\u0026rsquo;s better to use the playbook because it call the makinastates_pillar role to copy locally on the remote box the pillar computed by the salt+pillar bridge before executing the salt command."
}  , {"url": "/tags/cloud/", "title": "Cloud",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/cloud/", "title": "Cloud",  "subtitle": "", "section": "cloud", "tags": [], "type": "cloud", "description": "", "content": ""
}  , {"url": "/cloud/", "title": "Cloud Documentation",  "subtitle": "", "section": "cloud", "tags": ["cloud", "lxc"], "type": "cloud", "description": "", "content": "LXC"
}  , {"url": "/topics/configure/", "title": "Configuration",  "subtitle": "", "section": "topics", "tags": ["topics", "ansible"], "type": "topics", "description": "", "content": "Layout $wc is the makina-states top folder. bin/ansible - wrapper to ansible bin/ansible-galaxy - wrapper to ansible-galaxy bin/ansible-playbook - wrapper to ansible-playbook bin/salt-call - wrapper to salt-call ansible - ansible plays, roles, modules \u0026 etc etc - configuration etc/ansible - ansible configuration etc/salt - saltstack configuration etc/makina-states - makina-states configuration pillar - saltstack pillar files pillar/pillar.d - saltstack pillar files (global) pillar/private.pillar.d - saltstack pillar files (for the current node) pillar/.pillar.d - saltstack pillar files (for a specific minion) salt/makina-states - saltstack states salt/_modules - custom salt modules salt/_pillar - custom extpillar modules Salt pillar Saltstack configuration is based on pillars. To facilitate configuration of the Top file, we added those features: Any JSON file can be used as pillar data. Any SLS/json file dropped inside $wc/pillars.d/ will be loaded for all minion as pillar data Any SLS/json file dropped inside $wc/private.pillars.d will be only loaded for the current node of operation. Any SLS/json file dropped inside $wc/\u0026lt;$minionid\u0026gt;.pillars.d will be only loaded for the \u0026ldquo;\\$minionid\u0026rdquo; host Salt + Ansible bridge notes Makina-states has better to use an ansible dynamic inventory that bridges the salt pillar with ansible via a salt module: mc_remote_plllar. This module is pluggable and will search in the salt modules installed those who have declared special named functions: get_masterless_makinastates_hosts() return a list of host to manage get_masterless_makinastates_groups(minionid, pillar) return a list of groups for the specific minion id For each host found by all get_masterless_makinastates_hosts functions: Get its pillar by calling mc_remote_plllar.get_pillar($host) Extract/generate from informations in the pillar relevant ansible host vars for this minion. saltpillar ansible hostvar is the pillar of this minion. Generate ansible groups from those hostvars by calling eac get_masterless_makinastates_groups function By default, we use the mc_pillar ext pillar which loads a file: etc/makina-states/database.sls which describe our infractructure and this will: list all nodes that are configured as ansible targets generate pillar info for all nodes generate an ansible inventary for each of all those node Custom extpillar In other words, to add your custom way of managing your hosts: Create an ext_pillar to complete pillar for a specific minion depending on its minion id. Create a module that implement the get_masterless_makinastates_hosts \u0026amp;\u0026amp; get_masterless_makinastates_groups functions register the pillar and module to the local makina-states installation (see bellow) Take example on: module : (search for get_masterless_makinastates_groups \u0026amp;\u0026amp; get_masterless_makinastates_hosts extpillar To load your extpillar, you ll have to add it to the local salt configuration. You can add a file this way $WC/etc/salt/minion.d/99_extpillar.conf: ext_pillar: - mc_pillar: {} - mc_pillar_jsons: {} - mycustompillar: {} To load your custom module, place it under $WC/salt/_modules To load your custom pillar, place it under $WC/salt/_pillar Verify the pillar for a minion Use this command: bin/salt-call mc_remote_pillar.get_pillar \u0026lt;minion_id\u0026gt; Verify the groups for a minion Use this command: bin/salt-call mc_remote_pillar.get_groups \u0026lt;minion_id\u0026gt; (OPTIONAL) Add a cron to speed up pillar generation To generate regularly the cron for all the configured minion, to speed up regular ansible calls (the pillar will already be cached at the call time), you can register a cron that does that. /etc/cron.d/refresh_ansible 15,30,45,00 * * * * root /srv/makina-states/_scripts/refresh_makinastates_pillar.sh"
}  , {"url": "/tags/databasesls/", "title": "Databasesls",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/tags/haproxy/", "title": "Haproxy",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/topics/haproxy/", "title": "Haproxy",  "subtitle": "", "section": "topics", "tags": ["topics", "cloud", "haproxy"], "type": "topics", "description": "", "content": "the formula is in salt/makina-states/services/proxy/haproxy Idea We provide a facility to auto configure HTTP/HTTPS/REDIS/TCP backends via the informations that we can discover in the pillar. We want in an future iteration to add a discovery mecanism, maybe via the mine or a discovery service like ETCD, zookeeper or consul. The subkey for configuring proxies is the makina-states.haproxy_registrations top dictionnary. How to use Extra HAPROXY configuration can be done by overriding the default registry, mc_haproxy.settings via the makina-states.services.proxy.haproxy prefix as usual. Look available tweaks by calling salt-call mc_haproxy.settins. After, add makina-states.haproxy_registrations.* entries to add proxies. ssl is used and reused from makina-states.localsettings.ssl First, activate haproxy and run the states a first time: bin/salt-call -lall state.sls makina-states.services.proxy.haproxy The most complete form to register a minion with haproxy is as follow makina-states.haproxy_registrations.\u0026lt;arbitrar id\u0026gt;: - ip: \u0026lt;local_ip of backend\u0026gt; frontends: \u0026lt;frontend_port\u0026gt;: to_port: \u0026lt;dest_port\u0026gt;, # optional, default to \u0026lt;frontend_port\u0026gt; mode: \u0026lt;http/https/tcp/tcps/redis\u0026gt; hosts: [\u0026lt;hosts_to_proxy_from\u0026gt;] # only useful on http(s) mode regexes: [\u0026lt;hosts_to_proxy_from\u0026gt;] # only useful on http(s) mode wildcards: [\u0026lt;hosts_to_proxy_from\u0026gt;] # only useful on http(s) mode This declaration will help on the haproxy minion side be to configure appropriate haproxy frontends and backends objectds. Notes: The ip is the local ip of the minion to proxy requests to Frontends is a dictionnary of ports / metadata describing how to configure haproxy to proxy to the minion: mode is node the haproxy mode but a switch for us to know how to proxy requests. http/https Proxy HTTP(s) requests, depending on an additionnal regexes/wildcards/hosts knob regexes list of regexeses to match in the form [host_regex, PATH_URI_regex] wildcards list to strings which insensitive match if the header HOST endswith hosts list to strings which insensitive match exactly the header HOST tcp/tcps configure a tcp based proxy, here regexes/wildcards/hosts is useless. Be warn, main_cert will be the served ssl certicated as no SNI is possible redis configure a redis frontend (tcp based, so also no use of regexes/wildcards/hosts) based on https://support.pivotal.io/hc/en-us/articles/205309388-How-to-setup-HAProxy-and-Redis-Sentinel-for-automatic-failover-between-Redis-Master-and-Slave-servers to_port can be used to override the port to proxy on the minion side if it is not the same that on haproxy side If frontends are not specificied, you need to specify ip and one of hosts/regexes/wildcards as we default to configure http \u0026amp; https proxies. If frontends are specified, you need to respecify all of them, no default will be used in this case. 80 \u0026amp; 443 frontend port modes default to respectivly http \u0026amp; https. By default, if no frontends are specified, we setup http \u0026amp; https frontends. The SSL backend will try for forward first on 443, then on 80. Acls order for http mode is not predictible yet and will be difficult. Prefer to use a sensible configuration for your case rather than complicating the ACLS generation algorythm. proxy If we have a minion haproxy1 and want to proxy to myapp2-1 on http \u0026amp; https when a request targeting \u0026ldquo;www.super.com\u0026rdquo; arrise. all we have to do is to in haproxy1 pillar: makina-states.haproxy_registrations.haproxy1: - ip: 10.0.3.14 hosts: [www.super.com] wildcard Wilcards are also supported via the wildcards key makina-states.haproxy_registrations.haproxy1: - ip: 10.0.3.14 wildcards: [\u0026#39;*.www.super.com\u0026#39;] regex regex is also supported via the regexes key makina-states.haproxy_registrations.haproxy1: - ip: 10.0.3.14 regexes: [\u0026#39;my.*supemyappost.com\u0026#39;, \u0026#39;^/api\u0026#39;] if we want to proxy http to port \u0026ldquo;81\u0026rdquo; of myapp2-1 \u0026amp; https to 444 makina-states.haproxy_registrations.haproxy1: - ip: 10.0.3.14 hosts: [www.super.com] frontends: 80: {to_port: 81} 443: {to_port: 444} redis We have a special redis mode to do custom health checks on a redis cluster Short form if you use the default port on both ends: makina-states.haproxy_registrations.haredis: - ip: 10.0.3.14 # localip of myapp2-1 frontends: 6378: {} Long forms makina-states.haproxy_registrations.haredis: - ip: 10.0.3.14 frontends: 66378: {to_port: 666, mode: redis} 6378: {mode: redis} Redis auth is supported this way makina-states.haproxy_registrations.haredis: - ip: 10.0.3.14 frontends: 6378: {password: \u0026quot;foobar\u0026quot;, mode: redis} rabbitmq We have a special rabbitmq mode to set sane options on backend for rabbitmq Short form if you use the default port on both ends: makina-states.haproxy_registrations.haredis: - ip: 10.0.3.14 frontends: 5672: {} Long forms makina-states.haproxy_registrations.haredis: - ip: 10.0.3.14 frontends: 55672: {to_port: 333, mode: rabbitmq} 5672: {mode: rabbitmq} Register 2 backends for one same frondend makina-states.haproxy_registrations.mc_cloud_http1\u0026quot;: - \u0026quot;ip\u0026quot;: \u0026quot;10.5.5.2\u0026quot; \u0026quot;hosts\u0026quot;: [\u0026quot;es2.devhost5-1.local\u0026quot;] \u0026quot;frontends\u0026quot;: \u0026quot;80\u0026quot;: \u0026quot;mode\u0026quot;: \u0026quot;http\u0026quot; makina-states.haproxy_registrations.mc_cloud_http2\u0026quot;: - \u0026quot;ip\u0026quot;: \u0026quot;10.5.5.666\u0026quot; \u0026quot;hosts\u0026quot;: [\u0026quot;es2.devhost5-2.local\u0026quot;] \u0026quot;frontends\u0026quot;: \u0026quot;80\u0026quot;: \u0026quot;mode\u0026quot;: \u0026quot;http\u0026quot;"
}  , {"url": "/tags/installation/", "title": "Installation",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/topics/install/", "title": "Installation \u0026 basic usage",  "subtitle": "", "section": "topics", "tags": ["topics", "installation"], "type": "topics", "description": "", "content": "Briefing For now, use Ubuntu \u0026gt;= 14.04.. Makina-States can be ported to any linux based OS, but we here use ubuntu server and this is the only supported system for now. It can be used in any flavor, lxc, docker, baremetal, kvm, etc. To install our base salt installation, you have to choose between different modes of operations tweakable via the nodetype setting: The regular preset modes manages the system configuration from end to end, from the system, to makina-states, including the saltstack/salt/ansible installation itself. The special scratch mode manages only the saltstack + makina-states configuration by default, so it\u0026rsquo;s up to you to apply any other state but do not touch much to the system itself more than installing the scripts for wiring makina-states Makina-states do not use regular salt daemons to operate remotely but an ansible bridge that copy the pillar and use salt-call locally directly on the remote box Reminder Makina-states was based at fist on \u0026ldquo;nodetypes presets\u0026rdquo; that are prebundled collections of makina-states states to apply to the system. On those nodetypes, we manage \u0026ldquo;controllers\u0026rdquo;, aka the states which configure salt \u0026amp; ansible for operation On those nodetypes, we may configure \u0026ldquo;localsettings\u0026rdquo; like ssl, vim, git, \u0026amp; basepackages or network configurations. If any other preset than scratch has been activated, many localsettings will be applied by default (see mc_states/modules/localsettings.py:registry) After all of the previous steps, we may configure services like sshd, crond, or databases. If we are on the scratch mode, no services are configured by default. Eventually, we may by able to install projects via mc_project. A project is just a classical code repository which has a \u0026ldquo;.salt\u0026rdquo; and/or ansible playbooks/roles folder commited with enougth information on how to deploy it. Details Just run the _scrits/boot-salt.sh script as root, Please read next paragraphs before running any command. All the behavior of the script can be controlled via environment variables or command line arguments switches. You will need to tell on what kind of environment you are installing on (the nodetype). You\u0026rsquo;ll also have to set the minion id. The default choice for \u0026ndash;minion-id is the current machine hostname. You should keep this naming scheme unless you have a good reason to change it. Your choice for --nodetype is certainly one of: scratch manages by default only the salt installation and configuration. You ll want to activate this mode if you want to apply explicitly your states without relying of default nodetypes configuration. server (default) matches a baremetal server, and manage it from end to end (base packages, network, locales, sshd, crond, logrotate, etc, by default) vm matches a VM (not baremetal), this is mostly like server. lxccontainer matches a VM (not baremetal), this is mostly like server. laptop is like server but also install packages for working on a developement machine (prebacking a laptop for a dev) dockercontainer matches a VM (not baremetal), this is mostly like server, but install \u0026amp; preconfigure circus to manage daemons. devhost is suitable for a development machine enabling states to act on that, by example installation of a test local-loop mailer. vagrantvm is suitable to flag vagrant boxes and is a subtype of devhost Regular modes (via boot-salt.sh) boot-salt.sh will try to remember how you configured makina-states on each run. It stores configs in installdir/etc/makina-states Indeed while running, the script try to find enougth information (nodetype, salt installs, branch), and will automaticly guess \u0026amp; store the parameters by itself. In other words, you will just have to type boot-salt.sh, and verify settings the next time you ll use it. REMEMBER THAT FOR NOW YOU HAVE TO USE UBUNTU \u0026gt;= 14.04. Download Get the script: git clone http://raw.github.com/makinacorpus/makina-states Short overview: ./boot-salt.sh --help Detailed overview: ./boot-salt.sh --long-help CLI Exemples If you want to install only a minion which will be connected to a remote mastersalt master: ./boot-salt.sh If you want to manage from end to end your server, select also the laptop preset nodetype: ./boot-salt.sh -n laptop|server|kxccontainer|vm To skip the automatic code update/upgrade: ```sh ./boot-salt.sh -S ``` To switch on a makina-states branch, like the v2 branch in production: ```sh ./boot-salt.sh -b v2 ``` Upgrade Upgrade will: Run predefined \u0026amp; scheduled upgrade code Update makina-states repositories in /srv/salt \u0026amp; /srv/makina-states Update core repositories (like salt code source in /srv/makina-states/src/salt) Do the highstates (salt and masterone if any) boot-salt.sh -C --highstates Activating another nodetype preset after installation If you installed the scratch preset and want to switch to another preset: salt-call state.sls makina-states.nodetypes.\u0026lt;your_new_preset\u0026gt; If you installed a preset and want to switch to another preset: edit etc/makina-states/nodetype and put your new preset edit etc/makina-states/nodetypes.yaml and set to false your old preset Finally, run: salt-call state.sls makina-states.nodetypes.\u0026lt;your_new_preset\u0026gt;"
}   , {"url": "/cloud/lxc/", "title": "LXC containers management",  "subtitle": "", "section": "cloud", "tags": ["cloud", "lxc"], "type": "cloud", "description": "", "content": "workflow: WARNING currently only those backing store are supported/tested: dir overlayfs Go into makina-states folder: cd /srv/makina-states If wanted, edit database, specially vms, \u0026amp; cloud_vm_attrs: vim etc/makina-states/database.sls Define shell variable to copy/paste following commands: export controller=\u0026quot;$(hostname -f)\u0026quot; export cn=\u0026quot;$(hostname -f)\u0026quot; # export cn=\u0026quot;c.foo.net\u0026quot; export vm=\u0026quot;d.foo.net\u0026quot; export vm_tmpl=\u0026quot;makinastates\u0026quot; Preparation on the controller Refresh cache: # bin/salt-call -lall \\ # state.sls makina-states.services.cache.memcached # the first time service memcached restart _scripts/refresh_makinastates_pillar.sh # or with limit on hosts that the run will be ANSIBLE_TARGETS=\u0026quot;$controller,$vm,$cn\u0026quot; _scripts/refresh_makinastates_pillar.sh Preinstalling makina-states (controller \u0026amp; each compute node) Installing it: ANSIBLE_TARGETS=\u0026quot;$cn\u0026quot; bin/ansible-playbook \\ ansible/plays/makinastates/install.yml Configure the dns on a full makina-states infra with mc_pillar ANSIBLE_TARGETS=\u0026quot;$controller\u0026quot; bin/ansible-playbook \\ ansible/plays/cloud/controller.yml Compute node related part Configure compute_node with: ANSIBLE_TARGETS=\u0026quot;$cn\u0026quot; ansible-playbook \\ ansible/plays/cloud/compute_node.yml Cooking and delivery of container / container templates Initialise a lxc container that will be the base of our image (after creation go edit in it until sastified of the result): ANSIBLE_TARGETS=\u0026quot;$controller,lxc$vm_tmpl\u0026quot; bin/ansible-playbook \\ ansible/plays/cloud/create_container.yml Synchronise it to an offline image, this will copy the container to the image, and remove parts from it (like sshkeys) to impersonate it: ANSIBLE_TARGETS=\u0026quot;$controller\u0026quot; bin/ansible-playbook \\ ansible/plays/cloud/snapshot_container.yml -e \u0026quot;lxc_template=$vm_tmpllxc_container_name=lxc$vm_tmpl\u0026quot; Arguments: ANSIBLE_TARGETS : compute node where the container resides (must be in ansible inventary) lxc_template : lxc image to create lxc_container_name : lxc container which serve as a base for the image Transfer the template to the compute node where you want to spawn containers from that image: ANSIBLE_TARGETS=\u0026quot;$cn\u0026quot; ansible-playbook \\ ansible/plays/cloud/sync_container.yml \\ -e \u0026quot;lxc_orig_host=$controllerlxc_container_name=$vm_tmpl\u0026quot; Arguments: ANSIBLE_TARGETS : both orig and dest lxc_host : where to transfer container/template lxc_orig_host : where from transfer container/template lxc_container_name : lxc container to transfer Initialise a container Initialise and finish the container provisioning (from scratch) ANSIBLE_TARGETS=\u0026quot;$cn,$vm\u0026quot; bin/ansible-playbook \\ ansible/plays/cloud/create_container.yml Arguments: ANSIBLE_TARGETS : compute node where the container resides (must be in ansible inventary) \u0026amp; lxc container to create lxc_from_container : lxc container from which initing the container lxc_backing_store : (opt) backing store to use Initialise and finish the container provisioning (from template): ANSIBLE_TARGETS=\u0026quot;$cn,vm\u0026quot; bin/ansible-playbook \\ ansible/plays/cloud/create_container.yml -e \u0026quot;lxc_from_container=$vm_tmpl\u0026quot; Special case: use overlayfs to create the container: ANSIBLE_TARGETS=\u0026quot;$cn,$vm\u0026quot; bin/ansible-playbook \\ ansible/plays/cloud/create_container.yml \\ -e \u0026quot;lxc_from_container=$vm_tmpllxc_backing_store=overlayfs\u0026quot;"
}  , {"url": "/tags/lxc/", "title": "Lxc",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/", "title": "Makina States",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/topics/ssl/", "title": "Manage ssl certificates",  "subtitle": "", "section": "topics", "tags": ["topics", "ssl"], "type": "topics", "description": "", "content": "General use Management is done via the pillar, and via the makina-states.localsettings.ssl sls. After configuration, apply it to your system via bin/salt-call -l all state.sls makina-states.localsettings.ssl Or via ansible (remote host):: ANSIBLE_TARGETS=\u0026quot;$hostname\u0026quot; bin/ansible-playbook \\ ansible/plays/saltcall.yml -e saltargs makina-states.localsettings.ssl Add a certificate to the system trust system (add a \u0026lsquo;ca\u0026rsquo;) pillar/pillar.d/cert.sls # \u0026lt;CN\u0026gt; makina-states.localsettings.ssl.cas.foobar: | -----BEGIN CERTIFICATE----- MIIDejCCAmKgAwIBAgICA+gwDQYJKoZIhvcNAQELBQAwfzELMAkGA1UEBhMCRlIx DDAKBgNVBAgMA1BkTDEPMA0GA1UEBwwGTmFudGVzMRYwFAYDVQQKDA1NYWtpbmEg Q29ycHVzMQ8wDQYDVQQDDAZmb29iYXIxKDAmBgkqhkiG9w0BCQEWGWNvbnRhY3RA bWFraW5hLWNvcnB1cy5jb20wIBcNMTYwNTA0MTA0NjAzWhgPMzAzNDExMDQxMDQ2 MDNaMH8xCzAJBgNVBAYTAkZSMQwwCgYDVQQIDANQZEwxDzANBgNVBAcMBk5hbnRl czEWMBQGA1UECgwNTWFraW5hIENvcnB1czEPMA0GA1UEAwwGZm9vYmFyMSgwJgYJ KoZIhvcNAQkBFhljb250YWN0QG1ha2luYS1jb3JwdXMuY29tMIIBIjANBgkqhkiG 9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnLYWB4f9lRVc/fbqOvOCNTCefWnNwKehyf9z LKzZ93ki5bHYLKUoI7tWK2UOKNbnADhEfgGiWNcGtdrr9wc4FFLFR43tUfIxMfqe wUcsv06V9IsmIP4Pi+knAPZG5fXystlPfLjom4bCx5mQr2SGIijw2ogYHKAIdgZJ rviDWM2XIbdEx0TIqkOAokKqUtDr8ZEG289P5v5mrHjacAC8GzhxCgg1RWmaJOhW jc6bfdgLEOQCwt3hE92r+qrh0JjxBINVLE6IO8dL1jGxN8O+U/sQdhDvuN1bwyXd 8117+FSP8C+nnOK37MI27qv0D+sEZXZXAdEAY6w0WF4EAuY/kwIDAQABMA0GCSqG SIb3DQEBCwUAA4IBAQBHG6MkNhaeXWqMqzcYmLWZQZ6hONfRaK7lZlKmly6yVzLJ Y6v5wPMtWDzE0ALS0K2nhG4dEuEo1D1/dQhdz+zmJC5+xVnCzzWIxfNs0GgQMTVj eEmp3Hl0QBwe66swFaMKPz9+1eiQKaTE4pcwOXGEFwephaJWkswX4Fw0o9CA7NLl z0uIpHB12tcGlxS7joraj6aV4nKj+T3xVzsQqR2x5jbZMzsn/1W4afeSKZkBWiNI Z1cASST8OvDiBkQna7LNqDVfogezK0h/8Wbqp5dipeNIY9xu/L4Hr9+Djb9mOwp+ gKtKM4seNltkeKgYrupAUecK3Rs+xiF9j5xiv2X1 -----END CERTIFICATE----- Add a certificate and it\u0026rsquo;s key for reuse in makina-states packages applications The certicate can be either a selfsigned certificate or a certificate and its authority chain with the certificate itself comes first pillar/pillar.d/cert.sls # \u0026lt;CN\u0026gt; makina-states.localsettings.ssl.certificates.titi: - | -----BEGIN CERTIFICATE----- MIIDdjCCAl6gAwIBAgICA+gwDQYJKoZIhvcNAQELBQAwfTELMAkGA1UEBhMCRlIx DDAKBgNVBAgMA1BkTDEPMA0GA1UEBwwGTmFudGVzMRYwFAYDVQQKDA1NYWtpbmEg Q29ycHVzMQ0wCwYDVQQDDAR0aXRpMSgwJgYJKoZIhvcNAQkBFhljb250YWN0QG1h a2luYS1jb3JwdXMuY29tMCAXDTE2MDUwNDExMDcxNFoYDzMwMzQxMTA0MTEwNzE0 WjB9MQswCQYDVQQGEwJGUjEMMAoGA1UECAwDUGRMMQ8wDQYDVQQHDAZOYW50ZXMx FjAUBgNVBAoMDU1ha2luYSBDb3JwdXMxDTALBgNVBAMMBHRpdGkxKDAmBgkqhkiG 9w0BCQEWGWNvbnRhY3RAbWFraW5hLWNvcnB1cy5jb20wggEiMA0GCSqGSIb3DQEB AQUAA4IBDwAwggEKAoIBAQDMRnNmGHdSfjmv/HQSLKQ0aIhukNQJP1/pVYfoHN/X BE5pbbM3voZaecq5puJV1CAojYO9XkIY6FFOQnXbr+285ectuTHFHGTQpw/cEtUd uHR7SXdhcfFOMw2og/WcdiOj5+WqEkm5hsT5QWMFTYQxXsRtwWVxx9JzBSStPpzi aJ1bfg51F+iEFvnkAsYN6++CAGp93pKNhKKyPx52fiiSwVH5+7Iouw5BzX68DQK5 1i2YoHDRKdZmBJ/wVUgISsuHEf4JhKMfyiQWvfjqNL5FQx4nEnhHbcrYr+/h/2+A 7IdscdbpQa4mKK6+5B9EIjR/c/6LKmXhaQuNwg+UaP9JAgMBAAEwDQYJKoZIhvcN AQELBQADggEBAB/egp+ifuMfl7tFiRiO95QeIu6YLNSs6l2ZQ8uHSqlQ6/8GSq/J C1vt/yy4nPZ/14AonlIxWiKMH/1I96Y7W/8KZ5v9DbYsjGwO1TwqNxsqxjjMlW4g Qb1L4vnAq+25HhX0M1xiJWErgPfzMCVTyOhhuVaIPVTZUBhN5GsVtzuzeC4Vpg1O wAhBRgbyi9gxdWoxeaujColAoiwBYgLt6d+jg7I7RSYvd6bIixc00G0J4zY0d8jB ztK3UXbf4G0Bt7R/DcyZ0Tsp51+d5vpD+UjKkpijwhDkUGNC1ljD5M95NlmbCPdp 5ODKbWRuHLcUyzEAjzplwC6FpAlvN11SanA= -----END CERTIFICATE----- - | -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDMRnNmGHdSfjmv /HQSLKQ0aIhukNQJP1/pVYfoHN/XBE5pbbM3voZaecq5puJV1CAojYO9XkIY6FFO QnXbr+285ectuTHFHGTQpw/cEtUduHR7SXdhcfFOMw2og/WcdiOj5+WqEkm5hsT5 QWMFTYQxXsRtwWVxx9JzBSStPpziaJ1bfg51F+iEFvnkAsYN6++CAGp93pKNhKKy Px52fiiSwVH5+7Iouw5BzX68DQK51i2YoHDRKdZmBJ/wVUgISsuHEf4JhKMfyiQW vfjqNL5FQx4nEnhHbcrYr+/h/2+A7IdscdbpQa4mKK6+5B9EIjR/c/6LKmXhaQuN wg+UaP9JAgMBAAECggEAPkn9RlSPjggPbyp7+k7Cg3icoZpoDanVhUEfgBfN6bLW di+NRqJCNbSNrK7GtYVJiRQd59CmNxIgOMzrQ2ISDFfOdpLSKljOJRHMND9J3RYx 7qYoUP59pmrK72fNrTgZBhHgZkvNT1VZGuhlWWiZtrQ/EXi3hkp4UbpvxKQjEqZn o2+vbUYh440dKdw+iJnDAYZi/1yFCqiIxkWUSFFky8gjppQ7CBeQ4LEHKPodd4A2 ccnK8op6H5n26Xqdi65G0SR+X39xtEGp8pzUQW8XaXCjEtBNhD95ymGXEmq8KfqG XcN6nBMVp5V9X4+xFg2hBsFPsL6xrT7deASBKyDQAQKBgQD9VG8okA1rd11ttyvi DhIV6PlYBxhEex4lGeqOrjETKMsAFDsJ5Ntlr3MiXiLIuQ0pAcu3tpxNEkTLkcHo e0VpP7HCr14vnCpZyB1B8wTmbBJVMXkD+oaN7pzEJhauaombfDRH4NKd87lgveLG 3x/CFHdJfEvxwjRdpER+ZxJvIQKBgQDObaa9WcyZnuB7R109TTiZgSGo1XljF1m9 Ji+85sSdV1mHnUyghgszbnW5s8dM1sweY5ZAONFsF9j06iL2h1XnGlgKHBL22UGH o5nc5/oHfuc/YmRdSUCgmwSZixrInFjfvUiE/tR/j4z2AnpEB4/8genEm+elXYIE uodJpj3TKQKBgQChY+FNXjiudmU3OLLkWUJ8Yug3hI2ZUzZpPJGKRL9PDXYGntzd +MctiRE4m/BdIEeaEGLQr630C+d4KWv3yFD4NHPzK/Y9LqhsemjpUwGUKtWjINmQ B1MhqRqGfB2HEKiKPh6wjDKiHlvDnjWTrSJ2asN0NZPMeYUTA0v/m3rLAQKBgFJZ K8sdp6Eg4CxNq8RoqcuS1/qiLmp5RjNOqHyTEpwx3GVdOtROpOk/h3ctYLQmfAcj cyzrfZ/BY6tQO+Jc2sf2mmhuCqKuyJVzjk2xvOyAk3+VoLQWJNHtBUi7VVPyCwI2 YFet0NeSTIlXM68v1SDGMptcFmzBgLyiLJYU21UBAoGBAIhSvpsw3z/gAf9nbciZ 9puJiPqFBld7DrCp69iaD/ryXzLfzwI3bzWR8M8TuBO6DxApiYx7Zps4QabPQZTN U4UFg0AcdRh27OUXYtGENw7W0ssZKhlII78WB+0haAwe+kQJ4aNpF0eqWXLH7thR zKKdzi+lMlG5NimeR246wBvX -----END PRIVATE KEY----- Add certificates via the makina-states ext_pillar When using mc_pillar deployment, edit your local etc/makina-states/database.sls Take an example on the database.sl ssl \u0026amp; ssl_certs section. The ssl_certs section is a mapping id / tuples of (certicate, key) mappings. certicate can be either a selfsigned certificate or a certificate and all the authority, where the certificate of the common name comes first. The ssl section is where you will map certificates described in the ssl_certs section to a particular minion id. Remember that the special default section has the purpose to map certificate to any minion You can add either certificates for a host by specifying them by the id index or configure infra wide certs by setting them in the default section. Reconfigure the SSL system bin/salt-call -lall state.sls makina-states.localsettings.ssl Use the ssl macro in a state to register a certificate This add the certificate inside the cloud ssl directories Then also may add it to the systemwide ssl trust foo.sls: {% import \u0026quot;makina-states/localsettings/ssl/macros.jinja\u0026quot; as ssl with context %} {{ ssl.install_certificate( cert_string [, [cert_key_string]] [, trust=True/False]] )}} Parameters: cert_string : either a certificate string (full certificate in PEM format) or a path to load a certificate in PEM format or a key inside the mc_ssl.settings.certificates regitry (if you need an authority chain, place the certificate first) cert_key_string (optional) : in case of cert_string is neither a certificate inline or a certificate filepath, this will lookup inside the pillar for a matching certificate inside the mc_ssl.settings.certificates key. trust (optional) : boolean to tell to register the certificate to the system-wide ssl trusted certe"
}  , {"url": "/tags/minions/", "title": "Minions",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/topics/database/", "title": "Minions managment via database.sls",  "subtitle": "", "section": "topics", "tags": ["topics", "minions", "databasesls"], "type": "topics", "description": "", "content": "We use a special file: $WC/etcmakina-states/database.sls. for describing our infractrusture and specially how to access to the remote systems. At first, you will need to copy $WC/etc/makina-states/database.sls.in which is a sample, and adapt it to your needs: cp etc/makina-states/database.sls.in \\ etc/makina-states/database.sls $EDITOR etc/makina-states/database.sls The contents of the file is mostly self explainatory. This file is then parsed by the mc_pillar module (called via an extpillar hook) to get the appropriate pillar for a specific minion id that represant a good part of its system configuration from backups to ssl, to cloud configuration and so on. We heavyly rely on memcached to improve the performance, so first please install it this way: bin/salt-call -lall state.sls makina-states.services.cache.memcached Verify the pillar for a minion after a database.sls change Use this command: service memcached restart bin/salt-call mc_pillar.ext_pillar \u0026lt;minion_id\u0026gt;"
}  , {"url": "/tags/ssl/", "title": "Ssl",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/tags/", "title": "Tags",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/tags/topics/", "title": "Topics",  "subtitle": "", "section": "", "tags": [], "type": "page", "description": "", "content": ""
}  , {"url": "/topics/", "title": "Topics",  "subtitle": "", "section": "topics", "tags": [], "type": "topics", "description": "", "content": ""
}  , {"url": "/topics/", "title": "Topics",  "subtitle": "", "section": "topics", "tags": ["topics"], "type": "topics", "description": "", "content": "Ansible bridge Configuration database/extpillar haproxy Installation SSL"
}]
ent": "Ansible bridge Configuration database/extpillar haproxy Installation SSL"
}]
